version: '3.8'

services:
  # ollama:
  #   # image: ollama/ollama:latest  # Use the official Ollama image
  #   image: ollama/ollama  # Use the official Ollama image
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   command: >
  #     ollama pull nomic-embed-text:latest &&
  #     ollama pull mistral:latest &&
  #     ollama serve
  #   # command: serve  # Simplify the command to just serve models available
  #   environment:
  #     - MODELS=nomic-embed-text:latest,mistral:latest
  # ---------------------------Version 2-------------------------- >>> This version works well

  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - 11434:11434
  #   volumes:
  #     - .:/code
  #     - ./ollama/ollama:/root/.ollama
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   restart: always
  #   environment:
  #     - OLLAMA_KEEP_ALIVE=24h
  #     - OLLAMA_HOST=0.0.0.0
  #   command: -c "ollama pull nomic-embed-text:latest && ollama pull mistral:latest && ollama serve"
  # ---------------------------Version 2--------------------------

  # ---------------------------Version 3--------------------------

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - .:/code
      - ./ollama/ollama:/root/.ollama
    container_name: ollama
    entrypoint: ["/code/ollama-entrypoint.sh"]  # Use the script as the entrypoint
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
  # ---------------------------Version 3--------------------------
  chatbot:
    build: .
    container_name: chatbot
    environment:
      BASE_URL: http://host.docker.internal:11434
    ports:
      - "8501:8501"
    depends_on:
      - ollama
